{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-26T13:32:21.147006Z","iopub.execute_input":"2023-08-26T13:32:21.148067Z","iopub.status.idle":"2023-08-26T13:32:21.155155Z","shell.execute_reply.started":"2023-08-26T13:32:21.148023Z","shell.execute_reply":"2023-08-26T13:32:21.154202Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"os.mkdir(\"/kaggle/working/data\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:57:09.391888Z","iopub.execute_input":"2023-08-09T14:57:09.392266Z","iopub.status.idle":"2023-08-09T14:57:09.397265Z","shell.execute_reply.started":"2023-08-09T14:57:09.392233Z","shell.execute_reply":"2023-08-09T14:57:09.396187Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NJNischal/Facial-Expression-Recognition-with-CNNs.git","metadata":{"execution":{"iopub.status.busy":"2023-08-09T11:02:43.815103Z","iopub.execute_input":"2023-08-09T11:02:43.816011Z","iopub.status.idle":"2023-08-09T11:02:54.497673Z","shell.execute_reply.started":"2023-08-09T11:02:43.815976Z","shell.execute_reply":"2023-08-09T11:02:54.496219Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'Facial-Expression-Recognition-with-CNNs'...\nremote: Enumerating objects: 34122, done.\u001b[K\nremote: Counting objects: 100% (42/42), done.\u001b[K\nremote: Compressing objects: 100% (40/40), done.\u001b[K\nremote: Total 34122 (delta 19), reused 14 (delta 2), pack-reused 34080\u001b[K\nReceiving objects: 100% (34122/34122), 100.31 MiB | 15.75 MiB/s, done.\nResolving deltas: 100% (23/23), done.\nUpdating files: 100% (35915/35915), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.layers.serialization import activation\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, MaxPool2D,Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D\n\n\n# Load the pre-trained VGG19 model without top layers\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n#adding convulation layer\nx = Conv2D(32,(2,2))(base_model.output)\n#adding maxpooling layer\nx = MaxPool2D((2,2))(x)            # 122X122\n#adding the convulation layer\nx = Conv2D(64 , (2,2) )(x)\n#adding the maxpooling layer\nx = MaxPool2D((2,2))(x)            #61X61\n\n\n#flattening it\nx  = Flatten()(x)\n\n#neural network\nx = Dense(64, activation='relu')(x)\nx = Dense(128 , activation = 'relu')(x)\nx = Dense(128 , activation = 'relu')(x)\nx = Dense(64 , activation = 'relu')(x)\n\noutput = Dense(8, activation='softmax')(x)\n\nmodel = Model(inputs = base_model.input , outputs=output)\n\n# Compile the model\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n# loading  the dataset\ntrain_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input, validation_split=0.1)\n\n\ntrain_generator = train_datagen.flow_from_directory('/kaggle/input/affectnet-training-data', target_size=(224, 224),  class_mode='categorical', subset='training')\nvalidation_generator = train_datagen.flow_from_directory('/kaggle/input/affectnet-training-data', target_size=(224, 224), class_mode='categorical', subset='validation')\n\ntype(train_generator)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:57:25.108538Z","iopub.execute_input":"2023-08-09T14:57:25.108893Z","iopub.status.idle":"2023-08-09T14:58:09.600949Z","shell.execute_reply.started":"2023-08-09T14:57:25.108865Z","shell.execute_reply":"2023-08-09T14:58:09.600057Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94765736/94765736 [==============================] - 0s 0us/step\nFound 26142 images belonging to 8 classes.\nFound 2900 images belonging to 8 classes.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"keras.preprocessing.image.DirectoryIterator"},"metadata":{}}]},{"cell_type":"code","source":"# Training the model\nmodel.fit(train_generator, epochs=30, validation_data=validation_generator)\n\n# Evaluate the model on the validation data\nevaluation = model.evaluate(validation_generator)\n\n# Print the evaluation results\nprint(\"Validation Loss:\", evaluation[0])\nprint(\"Validation Accuracy:\", evaluation[1])","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:59:20.285448Z","iopub.execute_input":"2023-08-09T14:59:20.285855Z","iopub.status.idle":"2023-08-09T16:28:31.305314Z","shell.execute_reply.started":"2023-08-09T14:59:20.285821Z","shell.execute_reply":"2023-08-09T16:28:31.304305Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/30\n817/817 [==============================] - 267s 266ms/step - loss: 1.3253 - accuracy: 0.4595 - val_loss: 1.8621 - val_accuracy: 0.4045\nEpoch 2/30\n817/817 [==============================] - 160s 196ms/step - loss: 1.1059 - accuracy: 0.5564 - val_loss: 1.6479 - val_accuracy: 0.5334\nEpoch 3/30\n817/817 [==============================] - 159s 194ms/step - loss: 1.0150 - accuracy: 0.6005 - val_loss: 1.6748 - val_accuracy: 0.5241\nEpoch 4/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.9490 - accuracy: 0.6320 - val_loss: 1.4495 - val_accuracy: 0.5562\nEpoch 5/30\n817/817 [==============================] - 158s 194ms/step - loss: 0.8975 - accuracy: 0.6537 - val_loss: 1.6549 - val_accuracy: 0.5700\nEpoch 6/30\n817/817 [==============================] - 160s 195ms/step - loss: 0.8524 - accuracy: 0.6757 - val_loss: 1.6684 - val_accuracy: 0.5779\nEpoch 7/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.7965 - accuracy: 0.7014 - val_loss: 1.5954 - val_accuracy: 0.6262\nEpoch 8/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.7390 - accuracy: 0.7262 - val_loss: 1.3287 - val_accuracy: 0.6310\nEpoch 9/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.6832 - accuracy: 0.7492 - val_loss: 1.6539 - val_accuracy: 0.6238\nEpoch 10/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.6307 - accuracy: 0.7703 - val_loss: 1.4601 - val_accuracy: 0.6014\nEpoch 11/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.5630 - accuracy: 0.7973 - val_loss: 1.5479 - val_accuracy: 0.6328\nEpoch 12/30\n817/817 [==============================] - 158s 194ms/step - loss: 0.4892 - accuracy: 0.8248 - val_loss: 1.9686 - val_accuracy: 0.6038\nEpoch 13/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.4214 - accuracy: 0.8521 - val_loss: 2.0555 - val_accuracy: 0.6262\nEpoch 14/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.3717 - accuracy: 0.8693 - val_loss: 3.0192 - val_accuracy: 0.5848\nEpoch 15/30\n817/817 [==============================] - 157s 193ms/step - loss: 0.3046 - accuracy: 0.8948 - val_loss: 2.0443 - val_accuracy: 0.6390\nEpoch 16/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.2640 - accuracy: 0.9108 - val_loss: 2.4714 - val_accuracy: 0.6090\nEpoch 17/30\n817/817 [==============================] - 158s 194ms/step - loss: 0.2297 - accuracy: 0.9234 - val_loss: 2.5738 - val_accuracy: 0.6134\nEpoch 18/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.2055 - accuracy: 0.9324 - val_loss: 2.1642 - val_accuracy: 0.6331\nEpoch 19/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.1778 - accuracy: 0.9405 - val_loss: 2.7407 - val_accuracy: 0.6338\nEpoch 20/30\n817/817 [==============================] - 157s 192ms/step - loss: 0.1723 - accuracy: 0.9451 - val_loss: 2.7061 - val_accuracy: 0.6159\nEpoch 21/30\n817/817 [==============================] - 159s 194ms/step - loss: 0.1523 - accuracy: 0.9497 - val_loss: 2.9375 - val_accuracy: 0.6169\nEpoch 22/30\n817/817 [==============================] - 158s 194ms/step - loss: 0.1452 - accuracy: 0.9523 - val_loss: 3.5636 - val_accuracy: 0.6090\nEpoch 23/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.1443 - accuracy: 0.9525 - val_loss: 2.2784 - val_accuracy: 0.6393\nEpoch 24/30\n817/817 [==============================] - 159s 194ms/step - loss: 0.1281 - accuracy: 0.9597 - val_loss: 3.3953 - val_accuracy: 0.6200\nEpoch 25/30\n817/817 [==============================] - 159s 194ms/step - loss: 0.1172 - accuracy: 0.9636 - val_loss: 3.0082 - val_accuracy: 0.6241\nEpoch 26/30\n817/817 [==============================] - 159s 194ms/step - loss: 0.1111 - accuracy: 0.9656 - val_loss: 2.9596 - val_accuracy: 0.6038\nEpoch 27/30\n817/817 [==============================] - 159s 194ms/step - loss: 0.1176 - accuracy: 0.9616 - val_loss: 3.9436 - val_accuracy: 0.5959\nEpoch 28/30\n817/817 [==============================] - 159s 194ms/step - loss: 0.1145 - accuracy: 0.9635 - val_loss: 3.3029 - val_accuracy: 0.6017\nEpoch 29/30\n817/817 [==============================] - 159s 195ms/step - loss: 0.0932 - accuracy: 0.9709 - val_loss: 2.7106 - val_accuracy: 0.6207\nEpoch 30/30\n817/817 [==============================] - 158s 193ms/step - loss: 0.1083 - accuracy: 0.9639 - val_loss: 3.0157 - val_accuracy: 0.6183\n91/91 [==============================] - 9s 97ms/step - loss: 3.0157 - accuracy: 0.6183\nValidation Loss: 3.0157084465026855\nValidation Accuracy: 0.6182758808135986\n","output_type":"stream"}]},{"cell_type":"code","source":"os.chdir('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T16:39:06.204410Z","iopub.execute_input":"2023-08-09T16:39:06.205395Z","iopub.status.idle":"2023-08-09T16:39:06.210509Z","shell.execute_reply.started":"2023-08-09T16:39:06.205354Z","shell.execute_reply":"2023-08-09T16:39:06.209179Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\nmodel.save(\"visualmodel_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T16:39:10.497417Z","iopub.execute_input":"2023-08-09T16:39:10.497824Z","iopub.status.idle":"2023-08-09T16:39:11.566710Z","shell.execute_reply.started":"2023-08-09T16:39:10.497792Z","shell.execute_reply":"2023-08-09T16:39:11.565704Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#state of art image segmentation\n#UNet , Mask R , R CNN , SegNet , PSPNet\n# InceptionNetwork\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T11:03:08.586872Z","iopub.status.idle":"2023-08-09T11:03:08.587884Z","shell.execute_reply.started":"2023-08-09T11:03:08.587616Z","shell.execute_reply":"2023-08-09T11:03:08.587643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T16:39:39.944278Z","iopub.execute_input":"2023-08-09T16:39:39.944777Z","iopub.status.idle":"2023-08-09T16:39:40.959445Z","shell.execute_reply.started":"2023-08-09T16:39:39.944742Z","shell.execute_reply":"2023-08-09T16:39:40.958278Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"data  visualmodel_2.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink \nFileLink(r'/kaggle/working/visualmodel_2.h5')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T16:44:33.313771Z","iopub.execute_input":"2023-08-09T16:44:33.314148Z","iopub.status.idle":"2023-08-09T16:44:33.321535Z","shell.execute_reply.started":"2023-08-09T16:44:33.314116Z","shell.execute_reply":"2023-08-09T16:44:33.320323Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/visualmodel_2.h5","text/html":"<a href='/kaggle/working/visualmodel_2.h5' target='_blank'>/kaggle/working/visualmodel_2.h5</a><br>"},"metadata":{}}]}]}